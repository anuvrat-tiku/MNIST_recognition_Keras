{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import the MNIST Dataset\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAABxCAYAAADWFKcUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFGxJREFUeJzt3Xu0jNX/B/D3dkvSIZKU5VKOW0Lu/CwUUlKRkER00VKkfrG6nZSUa/kuRPmmcl1h5VKUUEghSxd9l0RUrrnfQtJX7e8f5zyf85nOjHNm5sw8M7Pfr7Ws9V7PmTPz6ZnpmX32fvbexloLIiIiIlcV8LsAIiIiIj+xMUREREROY2OIiIiInMbGEBERETmNjSEiIiJyGhtDRERE5DQ2hoiIiMhpKdEYMsasMsb8YYw5lfVvq981ucIYU8oYs8AYc9oYs9MYc7ffNbnIGJOe9f/ATL9rcYUxpr8x5itjzFljzFS/63GNMaaGMWaFMeaEMWa7MaaT3zW5whhzgTHmraxr/kljzEZjzM1+1xWNlGgMZelvrS2e9a+a38U4ZCKAPwGUBdADwOvGmGv8LclJEwFs8LsIx/wK4CUAb/tdiGuMMYUAvA9gMYBSAPoCmGmMqeprYe4oBGA3gJYASgDIADDXGFPJx5qikkqNIYozY8xFADoDeM5ae8pa+wWADwD09Lcytxhj7gJwHMCnftfiEmvtfGvtQgBH/K7FQdUBXAHgX9bav6y1KwCsAa89cWGtPW2tfcFau8Na+7e1djGAXwDU97u2SKVSY2iEMeawMWaNMaaV38U4oiqAc9baH9Wx7wCwZyhOjDFpAF4E8P9+10LkMwOglt9FuMgYUxaZ3wff+11LpFKlMfQkgKsAXAng3wAWGWOu9rckJxQH8Ns/jp0AcLEPtbhqGIC3rLV7/C6EKI62AjgIYLAxprAx5kZkDtkU87cs9xhjCgOYBWCatXaL3/VEKiUaQ9ba9dbak9bas9baacjsLm3vd10OOAUg7R/H0gCc9KEW5xhj6gJoA+BfftdCFE/W2v8C6AjgFgD7ATwBYC4A/lEQR8aYAgBmIPO+0f4+lxOVQn4XECMWmV2mFFs/AihkjEm31m7LOlYHSdxVmmRaAagEYJcxBsjsqStojKlpra3nY11EMWet/Q8ye4MAAMaYtQCm+VeRW0zmRectZE6eaZ/VQE1aSd8zZIwpaYxpZ4wpaowpZIzpAaAFgI/9ri3VWWtPA5gP4EVjzEXGmP8DcDsy/1Kg2Ps3gKsB1M369waADwG087MoV2Rdb4oCKIjMRmjRrFlOFAfGmNpZ57yYMWYQgHIApvpclkteB1ADwK3W2jN+FxOtpG8MASiMzOmthwAcBjAAQMd/3NRLsfMwgAuROX7/LoB+1lr2DMWBtfZ3a+1+7x8yhy3/sNYe8rs2R2QAOAPgKQD3ZOUMXytyS08A+5B57WkNoK219qy/JbnBGFMRwEPI/CNsv1rjr4fPpUXMWGv9roGIiIjIN6nQM0REREQUMTaGiIiIyGlsDBEREZHT2BgiIiIip7ExRERERE4La00MYwynnkXBWhvxQpA891E7bK0tE+kv8/xHjeffR7z2+IqffX/l6fyzZ4hcsdPvAhzH80+u4mffX3k6/2wMERERkdPYGCIiIiKnsTFERERETmNjiIiIiJzGxhARERE5jY0hIiIichobQ0REROQ0NoaIiIjIaWGtQE2UF/Xr15fcv39/yb169QIATJ8+XY5NmDBB8jfffBOH6oiIiAKxZ4iIiIicxsYQEREROc1Ym/c94BJlw7iCBQtKLlGiRK6P10M1xYoVAwBUq1ZNjj3yyCOSX3nlFcndu3eX/Mcff0geOXKk5KFDh+a17JTeLLFu3bqSV6xYITktLe28v3fixAnJpUuXzv/Csn1trW0Q6S8n+vnPD61bt5Y8a9YsyS1btpS8devWSJ+e5z9LRkaGZH39KFAg+2/TVq1aSf7ss8+ifs1UvvYkAWc++xdffLHk4sWLS77lllsAAGXKZO+XOnbsWMlnz56NZVl5Ov/sGSIiIiKnsTFERERETkuY2WQVKlSQXKRIEcnNmjUDADRv3lyOlSxZUnLnzp0jer09e/ZIHj9+vOROnTpJPnnypOTvvvtOcn50W6eCRo0aSZ43b55kPXSph2G98/nnn3/KMT001qRJE8l6Zpl+fDJo0aKFZP3ft2DBAj/KybOGDRtK3rBhg4+VpKbevXsDAJ588kk59vfffwd9bDi3LxDFW6VKlSTrz3PTpk0l16pV67zPUa5cOcmPPvpo/hUXIfYMERERkdPYGCIiIiKn+TpMFmoGUl5miEXK65bWMzpOnTolWc+i2bdvn+Rjx45JjmJGTVLyZuABQL169STPnDlTsu7yDGXbtm0AgNGjR8ux2bNnS16zZo1k/f6MGDEizIr9pWcCpaenS07EYTI9g6ly5cqSK1asKNmYiCcikeKd06JFi/pcSXJq3Lix5HvuuUeynu14zTXXBP3dQYMGAQB+/fVXOaZvvdDXsvXr10dfbIqoXr265Mcee0xyjx49JF944YWS9bVi9+7dkr1bJGrUqCHHunbtKnnSpEmSt2zZEm3ZEWHPEBERETmNjSEiIiJymq/DZLt27ZJ85MgRyZEOk+nuzePHj0u+/vrrJXszk2bMmBHRa7ho8uTJkvVClOHyhtj0Ylx6Zp4eXqpdu3bEr+M3bw82AFi3bp2PleROD28++OCDkvWwgV/d1qmgTZs2kgcMGJDj5/rcdujQQfKBAwdiW1iS6Natm+Rx48ZJvvTSSyXroZlVq1ZJ1gv8jRkzJsdz69/Tj73rrrsiLziJ6e/dUaNGAQg8/3pBxVC8WyEAoF27dpILFy4MIPDzrt9Dnf3CniEiIiJyGhtDRERE5DRfh8mOHj0qefDgwZJ1d/G3334LIHBhRG3jxo2S27ZtK/n06dOS9QyDgQMHRlGxW+rXrw8ge18ZIPTMIj3ctWjRIsl6rzdvJof3ngKBs/RuuOGGXF8nGegZWoluypQpQY/r7m4Kj56l9M4770gONvyvh2927twZ28ISXKFC2V9HDRpkbiX15ptvyjE9q3X16tWShw0bJvmLL76QfMEFF0ieO3cuAODGG28M+tpfffVVpGWnDL3g8AMPPJDn3/vpp58k6+9gPZusSpUqUVYXe8lz1SYiIiKKATaGiIiIyGkJszfZwoULJesFGL3FmurUqSPH7r//fsl6GEYPjWnff/+95L59+0ZfbArTC2EuX74cAJCWlibH9J5JS5YskaxnmelF0PTiid6QzKFDh+SY3vNN79Okh+b0Qo96z7JE482AK1u2rM+V5F2omZvee0/hu/feeyVfccUVOX6uZzxNnz49HiUlBb2QYrDhW/2Z1LOcfvvtt6DPpx8TbHhM7085bdq08IpNQV26dDnvz3fs2CFZ712o9ybTQ2OaXmwxUbFniIiIiJyWMD1DWrCW/okTJ4I+Vq+NMmfOHMmhdoOmnKpWrSpZ38ju9RocPnxYjuktSvRfU3pLkw8//DBoDode4v2JJ56QrJeBTzTt27cHEFh7ItI9V3oLDm3v3r3xKicl6HVS7rvvPsn6OuStffbSSy/Fr7AEp29+fuaZZyR7PdB6mwbdyxyqN0h79tlnz/tzvVO67q12lf4u9UZQli1bJse2b98u+eDBg2E9dzL0lrNniIiIiJzGxhARERE5LSGHyYJ54YUXJHvr3wCBN+vqpe919x7lpNfg0Dehe0M9QPbN63p7Cb0eR7yGgypUqBCX14lWtWrVchzTN+8nCv1+6+7rH3/8UbL33lNolSpVkjxv3rxcHz9hwgQAwMqVK2NVUlIYMmSIZD005m2VBABLly4FEHhz7pkzZ4I+X9GiRSXrG6X1dcNbt0wPUb7//vth157KvHXggMDv2/zQtGnTfH2+WGDPEBERETmNjSEiIiJyWtIMk+k1hPRd73rdGb10u+6K1kM7EydOBBC4Xo6LrrvuOsl6aEy7/fbbAQRutUHh0etxxIteF+qmm26S7K3jEmpLAj2zx5v5RKHpc+utMfVPn376qWS967prSpYsKfnhhx+WrK/D3tAYAHTs2PG8z6e3d5g1a5ZkfQuF9t577wEARo8enceK6Xz0TLyLLroo18dfe+21OY6tXbtW8rp16/KnsCiwZ4iIiIicxsYQEREROS1phsk0vUtu7969Jesdonv27Bk0e116ehl8vZCgK8aOHStZ7xCvh8TiPTymd3tPlUUzS5UqFdbj9bYz3vuiZ0mWL19ecpEiRSTrxSj1edQzcNavXw8AOHv2rBzTO4V//fXXYdXqIj18M3LkyKCP0Tun6605Qi0c6wL9WdULVGp66OWyyy4DAPTp00eO3XbbbZJr1aoluXjx4pL1sJvOM2fOBBB6yyYKVKxYMQBAzZo15djzzz8vOdStFbldw/WMNf3e/vXXX5EXm0/YM0REREROY2OIiIiInJaUw2TaggULJG/btk2yHgZq3bq15OHDhwMAKlasKMdefvllyam8J1OHDh0k693pdXfyBx98ENeaNN2tqmvauHGjH+WEzRuS0rW/8cYbkvUCc6HoWUneMNm5c+fk2O+//y558+bNkt9++23JevakHuo8cOAAgMDduvXCmVu2bMm1PheFu7jizz//LNk7567TCyrqfcDKlCkj+ZdffpGc22xfPdyi9ykrV66cZL2n4qJFi8Ks2A2FCxeWrGcYe59zfT71kLs+/3ommJ5h6Q21aXpY/o477pCsZ1rqz0o8sWeIiIiInMbGEBERETkt6YfJtE2bNknu2rWr5FtvvVWyN+PsoYcekmPp6emS27ZtG8sSfaWHRPTsjoMHD0qeM2dOzOvQ+6KF2gNnxYoVkp9++ulYl5QvvMXkdu7cKceaNWsW1nPs2rVL8sKFCwEAP/zwgxz78ssvI66vb9++AAKHJvSQDgWn98fKyyzHULPMXKYX8dQz8hYvXixZz7z0Zgzr/cOmTp0q+ejRo5Jnz54tWQ/r6OOUTV/79bDW/Pnzczx26NChkvU1ec2aNZL1+6Yfo2f8efS1Z8SIEZKDXfeAwJmvscaeISIiInIaG0NERETktJQaJtN0t+yMGTMkT5kyBUDgXe0tWrSQ3KpVK8mrVq2KXYEJRHdFxnIBSm94LCMjQ44NHjxYsp7l9Oqrr0o+depUzGqKhVGjRvldQlB6VqUnL7OjXOXNuAy1l5umh3O2bt0as5pSgbf4JxA4bBIOfc1u2bKlZD2MySHgbHrWmB760tdfbcmSJQCACRMmyDH9narft48++kiy3oNMzwrz9oTTQ2fe3pdA4P5yn3zyiWR9LT127FiOOvNzpjF7hoiIiMhpKdUzpNdoufPOOyU3bNhQsu4R8uj1WlavXh2j6hJXLNcW0usZeX+FdOvWTY7pv6g7d+4cszooOL1OFwVatmwZAOCSSy4J+nN9M7veFohiT08GCbU+mes3UBcsWFDysGHDJA8aNEiy3p7kqaeekuydO90b1KBBA8mvvfaaZL0+kV7rr1+/fpJXrlwJAEhLS5NjenKJ3k5Ib7uyfPnyIP9lwO7duwEAlStXDvrzSLBniIiIiJzGxhARERE5LSmHyapVqya5f//+kvXy3pdffvl5n0PvkqtvGk6V3dKD0bvT66zX/Rg4cGDUr/P4449Lfu655ySXKFECQODNcr169Yr69YhioXTp0gBCXxMmTZokOdlu8k92S5cu9buEhOetKwYEDo3pLX30envesDAANGnSBEDgzvI333yzZD1M+eKLL0r21vEDsoeyNL11yscffxw0d+/eXfLdd9+d4zmAwO+Y/MKeISIiInIaG0NERETktIQfJvOGu3TXmR4a0ztK54W3o7feqd7PndrjSc+00FkPKY4fP16ytxP6kSNH5JjXfQoAPXv2lFynTh3J5cuXl6yXWfe6tvXwAsWfHiKtWrWq5Gi2+kgVupu/QIHz/624du3aWJdDIbRr187vEhLekCFDgh7Xs8z0OkN6a6QqVaqc97n1Y/W2Gvr2k0i9++67QXOssWeIiIiInMbGEBERETktYYbJypYtK7lmzZqSvcWdqlevHtbz6SXfx4wZI9lb5C+VZ42FS3ebejuvA9mLIOoZAOnp6bk+nx4+8BbbAkJ321J86SHS3IaCXKAXBm3Tpo1k7xqhtxWYOHGi5AMHDsShOgrmqquu8ruEhLd//37JevsMb1skIPD2Bs3bYkMvQqx3k9+xY4fk/BgaSwS8EhIREZHT2BgiIiIip8V9mKxUqVKSJ0+eLFl3VYfTBaqHZPRO53pRrjNnzoRdZypat26d5A0bNkjWe7dp3iwzPYSp6Vlmeh+g/Fi4keKjadOmkqdOnepfIT4qWbKk5GCLte7du1eyXryO/PP5559L1kO9vP0hW4sWLSTrhXXr1asn+eDBg5K92cNA9g7xeog41bFniIiIiJzGxhARERE5LWbDZI0bN5asF3Zq1KiR5CuvvDLPz6f3U9ELAw4fPlzy6dOnw67TJXv27JGs93HT+9NkZGSc9znGjRsn+fXXX5e8ffv2/CiR4kAvukiUjDZt2iR527ZtkvUtFldffbXkQ4cOxaewBHLy5EnJM2bMCJopG3uGiIiIyGlsDBEREZHTYjZM1qlTp6A5lM2bN0tevHix5HPnzgEInCl2/Pjx/CjRafv27ZOs95nRmVLLkiVLAABdunTxuZLEsmXLFsl6dmrz5s39KIfCpG+VmDJlimS9/+SAAQMABH7PEGnsGSIiIiKnGb00f64PNibvD6YcrLUR37nKcx+1r621DSL9ZZ7/qPH8+yiVrz1paWmS586dK1lvrTJ//nwAQJ8+feRYHCfc8LPvrzydf/YMERERkdPYGCIiIiKncZgsjlK5qzoJsKvaXzz/PnLl2qOHzPQN1P369QMA1K5dW47F8WZqfvb9xWEyIiIiotywMURERERO4zBZHLnSVZ2g2FXtL55/H/Ha4yt+9v3FYTIiIiKi3LAxRERERE4LdzuOwwB2xqIQB1SM8vd57qPD8+8vnn//8Nz7i+ffX3k6/2HdM0RERESUajhMRkRERE5jY4iIiIicxsYQEREROY2NISIiInIaG0NERETkNDaGiIiIyGlsDBEREZHT2BgiIiIip7ExRERERE77H6hVg8WAk8kaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the first 5 digits\n",
    "# Matplotlib subplot(nrows, ncols, index, **kwargs)\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "for i in range(6):\n",
    "    ax = fig.add_subplot(1, 6, i+1, xticks = [], yticks = [])\n",
    "    ax.imshow(X_train[i], cmap = 'gray')\n",
    "    ax.set_title(str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 1 image (28, 28)\n",
      "Shape of input training samples (60000, 28, 28)\n",
      "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,  48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         54, 227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60,\n",
      "        224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252,\n",
      "        252, 252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253,\n",
      "        253, 190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252,\n",
      "        179,  12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,\n",
      "         84,   0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,\n",
      "         28,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,\n",
      "          0,   0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,\n",
      "          0,   0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85,\n",
      "        178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252,\n",
      "        252, 252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252,\n",
      "        233, 145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,\n",
      "         37,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0],\n",
      "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0]], dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "# Analyze an image\n",
    "import pprint\n",
    "print (\"Shape of 1 image\", X_train[1].shape)\n",
    "print (\"Shape of input training samples\", X_train.shape)\n",
    "pprint.pprint(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the X_train and X_test so that it is normalized between 0 and 1\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Shape of one label ()\n",
      "Shape of test labels dataset (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Analyse the labels\n",
    "print (y_test[1])\n",
    "print (\"Shape of one label\", y_train[1].shape)\n",
    "print (\"Shape of test labels dataset\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the test labels\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, 10)\n",
    "print (y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have to vectorize the input because the input data is a series of matrices.\n",
    "#### This includes flattening the input matrices.\n",
    "#### Since each input is 28*28, the flat vector length will be 786.\n",
    "#### This means, out input will have 786 nodes.\n",
    "#### We will add 2 hidden layers of 512 nodes each will have a ReLU activation unit.\n",
    "#### To combat overfitting, we will add 2 dropout layers. One between the two hidden layers and the other between hidden and output.\n",
    "#### The output unit will have 10 nodes, each corresponding to one class in the y label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = X_train[1].shape))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU helps with the vanishing gradient problem. It leaves all the positive values alone and pushes the negative values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our loss function is categorical cross entropy\n",
    "# Optimizer = rmsprop\n",
    "# Evaluation metric = accuracy\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  11.57 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test accuracy BEFORE TRAINING\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy = 100 * score[1]\n",
    "print ('Test accuracy: ', accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "#### We will be using a validation set in the training phase. This is not the testing set. The validation set is only used to calculate how well we are doing in the training phase. The validation data is not used to update the weights (not used in the backprop calculations). The reason for using a validation set is that the testing set is truly what the model has never seen before and can be used for actual evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.1114 - acc: 0.9668 - val_loss: 0.0941 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09409, saving model to MNIST_Keras_best.hdf5\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0784 - acc: 0.9769 - val_loss: 0.0894 - val_acc: 0.9741\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09409 to 0.08945, saving model to MNIST_Keras_best.hdf5\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0622 - acc: 0.9810 - val_loss: 0.0937 - val_acc: 0.9745\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.08945\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0529 - acc: 0.9841 - val_loss: 0.0923 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.08945\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0441 - acc: 0.9865 - val_loss: 0.1035 - val_acc: 0.9751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.08945\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0380 - acc: 0.9886 - val_loss: 0.0892 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08945 to 0.08919, saving model to MNIST_Keras_best.hdf5\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0339 - acc: 0.9896 - val_loss: 0.1146 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.08919\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0295 - acc: 0.9910 - val_loss: 0.1125 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08919\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.0262 - acc: 0.9917 - val_loss: 0.1115 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08919\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 44us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 0.1077 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08919\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Train the model\n",
    "# 20 % will be removed from training set and used as a validation set.\n",
    "# The model checkpoint class permits us to save the model weights after each epoch.\n",
    "# filepath is where we like to save the weights.\n",
    "# When we set verbose = 1, the text output during the training process will let us know when the weights file is updated.\n",
    "# After creating the checkpointer, it is passed to the model as a parameter. \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath = 'MNIST_Keras_best.hdf5', verbose = 1, save_best_only = True)\n",
    "hist = model.fit(X_train, y_train, batch_size = 128, epochs = 10, validation_split = 0.2, \n",
    "                 callbacks = [checkpointer], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy after training : 98.2 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model again\n",
    "score = model.evaluate(X_test, y_test, verbose = 0)\n",
    "accuracy = score[1] * 100\n",
    "\n",
    "# Print\n",
    "print (\"Test accuracy after training :\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
